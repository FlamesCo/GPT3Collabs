{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Visual Scripting Art CODEX 0.1.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FlamesLLC/GPT3Collabs/blob/main/Visual_Scripting_Art_CODEX_0_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toWe1IoH7X35"
      },
      "source": [
        "# Text to Image tool\n",
        " \n",
        "\n",
        "## Features \n",
        "* complex requests:\n",
        "  * image and/or text as main prompts  \n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QytcEMSKBtN-"
      },
      "source": [
        "**Run the cell below after each session restart**\n",
        "\n",
        "Mark `resume` and upload `.pt` file, if you're resuming from the saved params."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etzxXVZ_r-Nf",
        "cellView": "form"
      },
      "source": [
        "#@title General setup\n",
        "\n",
        "import subprocess\n",
        "CUDA_version = [s for s in subprocess.check_output([\"nvcc\", \"--version\"]).decode(\"UTF-8\").split(\", \") if s.startswith(\"release\")][0].split(\" \")[-1]\n",
        "print(\"CUDA version:\", CUDA_version)\n",
        "\n",
        "if CUDA_version == \"10.0\":\n",
        "    torch_version_suffix = \"+cu100\"\n",
        "elif CUDA_version == \"10.1\":\n",
        "    torch_version_suffix = \"+cu101\"\n",
        "elif CUDA_version == \"10.2\":\n",
        "    torch_version_suffix = \"\"\n",
        "else:\n",
        "    torch_version_suffix = \"+cu110\"\n",
        "\n",
        "!pip install torch==1.7.1{torch_version_suffix} torchvision==0.8.2{torch_version_suffix} -f https://download.pytorch.org/whl/torch_stable.html ftfy regex\n",
        "\n",
        "try: \n",
        "  !pip3 install googletrans==3.1.0a0\n",
        "  from googletrans import Translator, constants\n",
        "  translator = Translator()\n",
        "except: pass\n",
        "!pip install ftfy==5.8\n",
        "\n",
        "import os\n",
        "import io\n",
        "import time\n",
        "from math import exp\n",
        "import random\n",
        "import imageio\n",
        "import numpy as np\n",
        "import PIL\n",
        "from skimage import exposure\n",
        "from base64 import b64encode\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from IPython.display import HTML, Image, display, clear_output\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n",
        "import ipywidgets as ipy\n",
        "# import glob\n",
        "from google.colab import output, files\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "import clip\n",
        "model_vit, _ = clip.load('ViT-B/32')\n",
        "\n",
        "!pip install lpips\n",
        "import lpips\n",
        "\n",
        "!git clone https://github.com/eps696/aphantasia\n",
        "%cd /content/aphantasia/\n",
        "from clip_fft import to_valid_rgb, fft_image, slice_imgs, checkout\n",
        "from utils import pad_up_to, basename, img_list, img_read\n",
        "from progress_bar import ProgressIPy as ProgressBar\n",
        "\n",
        "workdir = '_out'\n",
        "tempdir = os.path.join(workdir, 'ttt')\n",
        "os.makedirs(tempdir, exist_ok=True)\n",
        "\n",
        "clear_output()\n",
        "\n",
        "resume = False #@param {type:\"boolean\"}\n",
        "if resume:\n",
        "  resumed = files.upload()\n",
        "  params_pt = list(resumed.values())[0]\n",
        "\n",
        "def makevid(seq_dir, size=None):\n",
        "  out_sequence = seq_dir + '/%03d.jpg'\n",
        "  out_video = seq_dir + '.mp4'\n",
        "  !ffmpeg -y -v warning -i $out_sequence $out_video\n",
        "  data_url = \"data:video/mp4;base64,\" + b64encode(open(out_video,'rb').read()).decode()\n",
        "  wh = '' if size is None else 'width=%d height=%d' % (size, size)\n",
        "  return \"\"\"<video %s controls><source src=\"%s\" type=\"video/mp4\"></video>\"\"\" % (wh, data_url)\n",
        "\n",
        "!nvidia-smi -L\n",
        "print('\\nDone!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbJ9K4Cq8MtB"
      },
      "source": [
        "Type some `text` and/or upload some image to start.  \n",
        "`fine_details` input would make micro details follow that topic.  \n",
        "Put to `subtract` the topics, which you would like to avoid in the result.  \n",
        "*NB: more prompts = more memory! (handled by auto-decreasing `samples` amount, hopefully you don't need to act).*  \n",
        "`invert` the whole criteria, if you want to see \"the totally opposite\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUvpdy8BWGuM",
        "cellView": "form"
      },
      "source": [
        "#@title Input\n",
        "\n",
        "text = \"\" #@param {type:\"string\"}\n",
        "fine_details = \"\" #@param {type:\"string\"}\n",
        "subtract = \"\" #@param {type:\"string\"}\n",
        "translate = False #@param {type:\"boolean\"}\n",
        "invert = False #@param {type:\"boolean\"}\n",
        "upload_image = False #@param {type:\"boolean\"}\n",
        "\n",
        "if translate:\n",
        "  text = translator.translate(text, dest='en').text\n",
        "if upload_image:\n",
        "  uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3Sj0fxmtw6K"
      },
      "source": [
        "`align` option is about composition (in fact, sampling distribution).  \n",
        "`sync` value adds SSIM loss between the output and input image (if there's one), allowing to \"redraw\" it with controlled similarity. \n",
        "\n",
        "Turn on `dual_model` to optimize with both CLIP models at once (eats more RAM!).  \n",
        "Decrease `samples` if you face OOM for higher resolutions (especially when several prompts are used with dual model).  \n",
        "Setting `steps` much higher (1000-..) will elaborate details and smoother tones, but may start throwing texts like graffiti."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nq0wA-wc-P-s"
      },
      "source": [
        "#@title Generate\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/GDrive')\n",
        "# clipsDir = '/content/GDrive/MyDrive/T2I ' + dtNow.strftime(\"%Y-%m-%d %H%M\")\n",
        "\n",
        "!rm -rf tempdir\n",
        "\n",
        "sideX = 1280 #@param {type:\"integer\"}\n",
        "sideY = 720 #@param {type:\"integer\"}\n",
        "#@markdown > Tweaks & tuning\n",
        "align = 'uniform' #@param ['central', 'uniform', 'overscan']\n",
        "dual_model = False #@param {type:\"boolean\"}\n",
        "sync =  0.01 #@param {type:\"number\"}\n",
        "#@markdown > Training\n",
        "steps = 200 #@param {type:\"integer\"}\n",
        "samples = 200 #@param {type:\"integer\"}\n",
        "learning_rate = .05 \n",
        "save_freq = 1 #@param {type:\"integer\"}\n",
        "\n",
        "if dual_model is True:\n",
        "  print(' using dual-model optimization')\n",
        "  model_rn, _ = clip.load('RN50')\n",
        "  samples = samples // 2\n",
        "if len(fine_details) > 0:\n",
        "  samples = int(samples * 0.9)\n",
        "if len(subtract) > 0:\n",
        "  samples = int(samples * 0.9)\n",
        "print(' using %d samples' % samples)\n",
        "\n",
        "norm_in = torchvision.transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
        "sign = 1. if invert is True else -1.\n",
        "\n",
        "if upload_image:\n",
        "  in_img = list(uploaded.values())[0]\n",
        "  print(' image:', list(uploaded)[0])\n",
        "  img_in = torch.from_numpy(imageio.imread(in_img).astype(np.float32)/255.).unsqueeze(0).permute(0,3,1,2).cuda()\n",
        "  in_sliced = slice_imgs([img_in], samples, 224, norm_in)[0]\n",
        "  img_enc = model_vit.encode_image(in_sliced).detach().clone()\n",
        "  if dual_model is True:\n",
        "    img_enc = torch.cat((img_enc, model_rn.encode_image(in_sliced).detach().clone()), 1)\n",
        "  if sync > 0:\n",
        "    sim_loss = lpips.LPIPS(net='vgg', verbose=False).cuda()\n",
        "    img_in = F.interpolate(img_in, (sideY, sideX)).float()\n",
        "  else:\n",
        "    del img_in\n",
        "  del in_sliced; torch.cuda.empty_cache()\n",
        "\n",
        "if len(text) > 2:\n",
        "  print(' macro:', text)\n",
        "  if translate:\n",
        "    translator = Translator()\n",
        "    text = translator.translate(text, dest='en').text\n",
        "    print(' translated to:', text) \n",
        "  tx = clip.tokenize(text)\n",
        "  txt_enc = model_vit.encode_text(tx.cuda()).detach().clone()\n",
        "  if dual_model is True:\n",
        "    txt_enc = torch.cat((txt_enc, model_rn.encode_text(tx.cuda()).detach().clone()), 1)\n",
        "\n",
        "if len(fine_details) > 0:\n",
        "  print(' micro:', fine_details)\n",
        "  if translate:\n",
        "      translator = Translator()\n",
        "      fine_details = translator.translate(fine_details, dest='en').text\n",
        "      print(' translated to:', fine_details) \n",
        "  tx2 = clip.tokenize(fine_details)\n",
        "  txt_enc2 = model_vit.encode_text(tx2.cuda()).detach().clone()\n",
        "  if dual_model is True:\n",
        "      txt_enc2 = torch.cat((txt_enc2, model_rn.encode_text(tx2.cuda()).detach().clone()), 1)\n",
        "\n",
        "if len(subtract) > 0:\n",
        "  print(' without:', subtract)\n",
        "  if translate:\n",
        "      translator = Translator()\n",
        "      subtract = translator.translate(subtract, dest='en').text\n",
        "      print(' translated to:', subtract) \n",
        "  tx0 = clip.tokenize(subtract)\n",
        "  txt_enc0 = model_vit.encode_text(tx0.cuda()).detach().clone()\n",
        "  if dual_model is True:\n",
        "      txt_enc0 = torch.cat((txt_enc0, model_rn.encode_text(tx0.cuda()).detach().clone()), 1)\n",
        "\n",
        "shape = [1, 3, sideY, sideX]\n",
        "param_f = fft_image \n",
        "# param_f = pixel_image\n",
        "# learning_rate = 1.\n",
        "params, image_f = param_f(shape)\n",
        "image_f = to_valid_rgb(image_f)\n",
        "optimizer = torch.optim.Adam(params, learning_rate)\n",
        "\n",
        "def save_img(img, fname=None):\n",
        "  img = np.array(img)[:,:,:]\n",
        "  img = np.transpose(img, (1,2,0))  \n",
        "  img = np.clip(img*255, 0, 255).astype(np.uint8)\n",
        "  if fname is not None:\n",
        "    imageio.imsave(fname, np.array(img))\n",
        "    imageio.imsave('result.jpg', np.array(img))\n",
        "\n",
        "def checkout(num):\n",
        "  with torch.no_grad():\n",
        "    img = image_f().cpu().numpy()[0]\n",
        "    if sync > 0 and upload_image:\n",
        "      img = img **1.5 # empirical tone mapping\n",
        "  save_img(img, os.path.join(tempdir, '%03d.jpg' % num))\n",
        "  outpic.clear_output()\n",
        "  with outpic:\n",
        "    display(Image('result.jpg'))\n",
        "\n",
        "def train(i):\n",
        "  loss = 0\n",
        "  img_out = image_f()\n",
        "\n",
        "  micro = False if len(fine_details) > 0 else None\n",
        "  imgs_sliced = slice_imgs([img_out], samples, 224, norm_in, align, micro=micro)\n",
        "  out_enc = model_vit.encode_image(imgs_sliced[-1])\n",
        "  if dual_model is True: # use both clip models\n",
        "      out_enc = torch.cat((out_enc, model_rn.encode_image(imgs_sliced[-1])), 1)\n",
        "  if upload_image:\n",
        "      loss += sign * torch.cosine_similarity(img_enc, out_enc, dim=-1).mean()\n",
        "  if len(text) > 0: # input text\n",
        "      loss += sign * torch.cosine_similarity(txt_enc, out_enc, dim=-1).mean()\n",
        "  if len(subtract) > 0: # subtract text\n",
        "      loss += -sign * torch.cosine_similarity(txt_enc0, out_enc, dim=-1).mean()\n",
        "  if sync > 0 and upload_image: # image composition sync\n",
        "      prog_sync = (steps - i) / steps \n",
        "      loss += prog_sync * sync * sim_loss(F.interpolate(img_out, sim_size).float(), img_in, normalize=True).squeeze()\n",
        "  if len(fine_details) > 0: # input text for micro details\n",
        "      imgs_sliced = slice_imgs([img_out], samples, norm_in, align, micro=True)\n",
        "      out_enc2 = model_vit.encode_image(imgs_sliced[-1])\n",
        "      if dual_model is True:\n",
        "          out_enc2 = torch.cat((out_enc2, model_rn.encode_image(imgs_sliced[-1])), 1)\n",
        "      loss += sign * torch.cosine_similarity(txt_enc2, out_enc2, dim=-1).mean()\n",
        "      del out_enc2; torch.cuda.empty_cache()\n",
        "  del img_out, imgs_sliced, out_enc; torch.cuda.empty_cache()\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  \n",
        "  if i % save_freq == 0:\n",
        "    checkout(i // save_freq)\n",
        "\n",
        "outpic = ipy.Output()\n",
        "outpic\n",
        "\n",
        "pbar = ProgressBar(steps)\n",
        "for i in range(steps):\n",
        "  train(i)\n",
        "  _ = pbar.upd()\n",
        "\n",
        "HTML(makevid(tempdir))\n",
        "torch.save(params, tempdir + '.pt')\n",
        "files.download(tempdir + '.pt')\n",
        "files.download(tempdir + '.mp4')\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}